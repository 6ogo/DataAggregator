{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e522d09-5218-4d13-8807-cb4c6fac1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started...\n",
      "Configuration Loaded. Data Dir: /mnt/c/users/b816i1/git/reports/lf-mmm/code/PYMC MMM/2025-04-10\n",
      "\n",
      "--- Initializing Base Daily DataFrame ---\n",
      "\n",
      "--- Processing Transfers (../Data/simulated_car_transfer.xlsx) ---\n",
      "Info (Transfers): Loaded 34023 rows.\n",
      "Info (Transfers): 25842 rows remain after date cleaning/filtering.\n",
      "Info (Transfers): Cleaning regions...\n",
      "Info (Transfers): 24686 rows remain after region cleaning/filtering.\n",
      "Info (Transfers): Aggregating ['Ownership_Transfers'] by ['Date']...\n",
      "Info (Transfers): Processing complete. Result has 1179 rows.\n",
      "\n",
      "--- Processing Cpi (../Data/CPI.xlsx) ---\n",
      "Info (Cpi): Loaded 38 rows.\n",
      "Info (Cpi): 38 rows remain after date cleaning/filtering.\n",
      "Info (Cpi): Processed as national indicator. 38 unique date entries.\n",
      "\n",
      "--- Processing Sales (../Data/Försäljning Personbil.csv) ---\n",
      "Info (Sales): Loaded 28233 rows.\n",
      "Info (Sales): 28233 rows remain after date cleaning/filtering.\n",
      "Info (Sales): Cleaning regions...\n",
      "Info (Sales): 27116 rows remain after region cleaning/filtering.\n",
      "Info (Sales): Aggregating ['Sales'] by ['Date']...\n",
      "Info (Sales): Processing complete. Result has 1179 rows.\n",
      "\n",
      "--- Processing Email (../Data/EMMAUtskick.csv) ---\n",
      "Info (Email): Loaded 3721 rows.\n",
      "Info (Email): 3713 rows remain after date cleaning/filtering.\n",
      "Info (Email): Cleaning regions...\n",
      "Info (Email): 0 rows remain after region cleaning/filtering.\n",
      "Warning (Email): No data remains after filtering for valid regions.\n",
      "\n",
      "--- Processing Marketing (../Data/MMM cleaned 2503.csv) ---\n",
      "Info (Marketing): Loaded 70891 rows.\n",
      "Info (Marketing): 70891 rows remain after date cleaning/filtering.\n",
      "Info (Marketing): Cleaning regions...\n",
      "Info (Marketing): 64842 rows remain after region cleaning/filtering.\n",
      "Info (Marketing): Aggregating ['Marketing_Impressions', 'Cost'] by ['Date', 'Channel']...\n",
      "Info (Marketing): Pivoting marketing data...\n",
      "Info (Marketing): Pivoting successful. Columns: ['Date', 'Display Prospecting_Cost', 'LinkedIn Prospecting_Cost', 'Meta Prospecting_Cost', 'Meta Retargeting_Cost', 'SEM Prospecting_Cost', 'Display Prospecting_Impressions', 'LinkedIn Prospecting_Impressions', 'Meta Prospecting_Impressions', 'Meta Retargeting_Impressions', 'SEM Prospecting_Impressions']\n",
      "Info (Marketing): Processing complete. Result has 1179 rows.\n",
      "\n",
      "--- Processing Fuel (../Data/Bränslepriser.xlsx) ---\n",
      "Info (Fuel): Loaded 39 rows.\n",
      "Info (Fuel): 39 rows remain after date cleaning/filtering.\n",
      "Info (Fuel): Processed as national indicator. 39 unique date entries.\n",
      "\n",
      "--- Merging Processed Data ---\n",
      "Merging transfers using left merge...\n",
      "Merging cpi using merge_asof...\n",
      "Merging sales using left merge...\n",
      "Merging marketing using left merge...\n",
      "Merging fuel using merge_asof...\n",
      "\n",
      "Columns after merging: ['Date', 'Ownership_Transfers', 'CPI', 'Sales', 'Display Prospecting_Cost', 'LinkedIn Prospecting_Cost', 'Meta Prospecting_Cost', 'Meta Retargeting_Cost', 'SEM Prospecting_Cost', 'Display Prospecting_Impressions', 'LinkedIn Prospecting_Impressions', 'Meta Prospecting_Impressions', 'Meta Retargeting_Impressions', 'SEM Prospecting_Impressions', 'Gas_Price']\n",
      "\n",
      "--- Generating Daily Flags ---\n",
      "Generating Holiday flags...\n",
      "Fetching Swedish holidays for years: [2022, 2023, 2024, 2025]\n",
      "Found 260 official holiday dates.\n",
      "Holiday flags generated. Found 208 holidays in date range.\n",
      "Sample of dates marked as holidays: DatetimeIndex(['2022-01-01', '2022-01-02', '2022-01-06', '2022-01-09',\n",
      "               '2022-01-16'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Generating Ramadan flags...\n",
      "Ramadan flags generated. Count: 116\n",
      "\n",
      "--- Filling Final NaNs ---\n",
      "Warning: Expected summed/pivoted column 'Email_Impressions' not present for fillna(0).\n",
      "\n",
      "--- Aggregating to Weekly Level ---\n",
      "Weekly aggregation functions (Count: 16): ['Ownership_Transfers', 'CPI', 'Sales', 'Display Prospecting_Cost', 'LinkedIn Prospecting_Cost', 'Meta Prospecting_Cost', 'Meta Retargeting_Cost', 'SEM Prospecting_Cost', 'Display Prospecting_Impressions', 'LinkedIn Prospecting_Impressions', 'Meta Prospecting_Impressions', 'Meta Retargeting_Impressions', 'SEM Prospecting_Impressions', 'Gas_Price', 'Is_Holiday', 'Is_Ramadan']\n",
      "Weekly aggregation complete. Result shape: (169, 16)\n",
      "            Ownership_Transfers  CPI  Sales  Display Prospecting_Cost  \\\n",
      "Date                                                                    \n",
      "2022-01-03                 5769  3.7   3105                      0.36   \n",
      "2022-01-10                19582  3.7   5685                   6015.67   \n",
      "2022-01-17                21886  3.7   5884                  40657.11   \n",
      "2022-01-24                21649  3.7   5975                  41256.49   \n",
      "2022-01-31                25049  3.7   7682                  41698.10   \n",
      "\n",
      "            LinkedIn Prospecting_Cost  Meta Prospecting_Cost  \\\n",
      "Date                                                           \n",
      "2022-01-03                        0.0                   0.00   \n",
      "2022-01-10                        0.0                2325.63   \n",
      "2022-01-17                        0.0               18819.61   \n",
      "2022-01-24                        0.0               19081.29   \n",
      "2022-01-31                        0.0               19070.99   \n",
      "\n",
      "            Meta Retargeting_Cost  SEM Prospecting_Cost  \\\n",
      "Date                                                      \n",
      "2022-01-03                    0.0              29947.24   \n",
      "2022-01-10                    0.0              97428.06   \n",
      "2022-01-17                    0.0              83424.78   \n",
      "2022-01-24                    0.0              87240.14   \n",
      "2022-01-31                    0.0              83229.87   \n",
      "\n",
      "            Display Prospecting_Impressions  LinkedIn Prospecting_Impressions  \\\n",
      "Date                                                                            \n",
      "2022-01-03                            522.0                               0.0   \n",
      "2022-01-10                         123713.0                               0.0   \n",
      "2022-01-17                        1203148.0                               0.0   \n",
      "2022-01-24                        1008535.0                               0.0   \n",
      "2022-01-31                         929049.0                               0.0   \n",
      "\n",
      "            Meta Prospecting_Impressions  Meta Retargeting_Impressions  \\\n",
      "Date                                                                     \n",
      "2022-01-03                           0.0                           0.0   \n",
      "2022-01-10                      155990.0                           0.0   \n",
      "2022-01-17                      955895.0                           0.0   \n",
      "2022-01-24                      908313.0                           0.0   \n",
      "2022-01-31                      812912.0                           0.0   \n",
      "\n",
      "            SEM Prospecting_Impressions  Gas_Price  Is_Holiday  Is_Ramadan  \n",
      "Date                                                                        \n",
      "2022-01-03                       5332.0      18.48           1           0  \n",
      "2022-01-10                      19795.0      18.48           1           0  \n",
      "2022-01-17                      18882.0      18.48           1           0  \n",
      "2022-01-24                      19048.0      18.48           1           0  \n",
      "2022-01-31                      17175.0      18.48           1           0  \n",
      "\n",
      "--- Saving Results ---\n",
      "Successfully saved aggregated weekly data to /mnt/c/users/b816i1/git/reports/lf-mmm/code/PYMC MMM/2025-04-10/aggregated_weekly_data.xlsx\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import datetime\n",
    "import warnings\n",
    "import re\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "print(\"Script started...\")\n",
    "\n",
    "# %% ----- Configuration -----\n",
    "# Use Path object relative to the script location\n",
    "SCRIPT_DIR = Path(__file__).parent if \"__file__\" in locals() else Path.cwd()\n",
    "DATA_DIR = SCRIPT_DIR # Assume data files are in the same dir as the script\n",
    "# Example if data is in a 'Data' subdirectory:\n",
    "# DATA_DIR = SCRIPT_DIR / \"Data\"\n",
    "# if not DATA_DIR.exists():\n",
    "#    raise FileNotFoundError(f\"Could not find the data directory: {DATA_DIR.resolve()}\")\n",
    "\n",
    "# File Names (relative to DATA_DIR)\n",
    "FILE_NAMES = {\n",
    "    'transfers': '../Data/simulated_car_transfer.xlsx',\n",
    "    'cpi': '../Data/CPI.xlsx',\n",
    "    'sales': '../Data/Försäljning Personbil.csv',\n",
    "    'email': '../Data/EMMAUtskick.csv',\n",
    "    'marketing': '../Data/MMM cleaned 2503.csv',\n",
    "    'fuel': '../Data/Bränslepriser.xlsx'\n",
    "}\n",
    "\n",
    "# Date Range\n",
    "START_DATE = pd.to_datetime('2022-01-01')\n",
    "END_DATE = pd.to_datetime('2025-03-24')\n",
    "\n",
    "# Define valid regions (Swedish regions) - From your example\n",
    "VALID_REGIONS = [\n",
    "    'Stockholm', 'Göteborg och Bohuslän', 'Skåne', 'Uppsala', 'Östgöta', 'Södermanland',\n",
    "    'Halland', 'Kalmar', 'Kronoberg', 'Blekinge', 'Gotland', 'Värmland', 'Dalarna',\n",
    "    'Gävleborg', 'Västernorrland', 'Jämtland', 'Västerbotten', 'Norrbotten', 'Jönköping',\n",
    "    'Älvsborg', 'Skaraborg', 'Bergslagen', 'Göinge'\n",
    "]\n",
    "VALID_REGIONS_SET = set(VALID_REGIONS) # Use set for faster lookups\n",
    "\n",
    "# Define standard column names expected *after* cleaning in this script\n",
    "# We map RAW names from files to these standard names used internally.\n",
    "STD_COL_MAPPINGS = {\n",
    "    # Standard Name : [List of possible raw names in input files]\n",
    "    'Date': ['date', 'Date', 'DATE', 'datum', 'day'],\n",
    "    'Region': ['region', 'Region', 'REGION', 'Bolag', 'bolag', 'län'],\n",
    "    'Sales': ['avtal', 'Avtal', 'sales', 'försäljning'], # Target for 'Raw_Sales'\n",
    "    'Cost': ['cost', 'Cost', 'spend', 'Spend', 'utgift'], # Target for 'Raw_Cost'\n",
    "    'Channel': ['channel_grouping', 'Channel_Grouping', 'channel', 'kanal'], # Target for 'Raw_Channel'\n",
    "    # *** NOTE: Raw marketing impressions map to this standard name INTERNALLY before pivoting ***\n",
    "    'Marketing_Impressions': ['impressions', 'Impressions', 'visningar'],\n",
    "    'Ownership_Transfers': ['transfers', 'ownership_transfers', 'överföringar'], # Target for 'Raw_Transfers'\n",
    "    'Gas_Price': ['bensin', 'Bensin', 'fuel_price', 'Fuel_Price', 'bränslepris'], # Target for 'Raw_Fuel_Price'\n",
    "    'CPI': ['cpi', 'CPI', 'kpi'], # Target for 'Raw_CPI'\n",
    "    'Email_Impressions': ['email_opens', 'opens', 'eimpressions', 'eImpressions'], # Target for 'Raw_Email_Impressions'\n",
    "}\n",
    "\n",
    "# Ramadan Dates\n",
    "RAMADAN_PERIODS = [\n",
    "    ('2022-04-02', '2022-05-01'),\n",
    "    ('2023-03-22', '2023-04-20'),\n",
    "    ('2024-03-10', '2024-04-09'),\n",
    "    ('2025-02-28', '2025-03-29')\n",
    "]\n",
    "\n",
    "print(f\"Configuration Loaded. Data Dir: {DATA_DIR.resolve()}\")\n",
    "\n",
    "# %% ----- Utility Functions (Adapted from your example) -----\n",
    "\n",
    "def clean_region(region):\n",
    "    \"\"\"Standardize region names to match VALID_REGIONS list\"\"\"\n",
    "    if pd.isna(region): return 'Övrigt'\n",
    "    region_str = str(region).strip()\n",
    "    region_lower = region_str.lower()\n",
    "    if region_lower in ['nationell', 'nationwide', 'all', 'övrig', 'ovrigt', 'unknown', 'okänt', 'kluster', 'sak', 'is_national_row']: return 'Övrigt'\n",
    "    if region_lower.endswith(' län') or region_lower.endswith(' lan'): region_lower = region_lower[:-4].strip()\n",
    "    if '-' in region_lower and 'göinge' in region_lower: region_lower = 'göinge'\n",
    "    special_cases = {\n",
    "        'alvsborg': 'Älvsborg', 'älvsborg': 'Älvsborg', 'bergslagen': 'Bergslagen', 'västmanlands': 'Bergslagen',\n",
    "        'örebro': 'Bergslagen', 'blekinge': 'Blekinge', 'dalarna': 'Dalarna', 'dalarnas': 'Dalarna', 'gavleborg': 'Gävleborg',\n",
    "        'gävleborg': 'Gävleborg', 'gävleborgs': 'Gävleborg', 'goinge': 'Göinge', 'göinge': 'Göinge', 'goinge-kristianstad': 'Göinge',\n",
    "        'göinge-kristianstad': 'Göinge', 'goteborg': 'Göteborg och Bohuslän', 'göteborg': 'Göteborg och Bohuslän',\n",
    "        'goteborg och bohus': 'Göteborg och Bohuslän', 'göteborg och bohus': 'Göteborg och Bohuslän', 'göteborg & bohus': 'Göteborg och Bohuslän',\n",
    "        'goteborg-och-bohuslan': 'Göteborg och Bohuslän', 'göteborg och bohuslän': 'Göteborg och Bohuslän', 'västra götalands': 'Göteborg och Bohuslän',\n",
    "        'gotland': 'Gotland', 'gotlands': 'Gotland', 'halland': 'Halland', 'hallands': 'Halland', 'jamtland': 'Jämtland',\n",
    "        'jämtland': 'Jämtland', 'jämtlands': 'Jämtland', 'jonkoping': 'Jönköping', 'jönköping': 'Jönköping', 'jönköpings': 'Jönköping',\n",
    "        'kalmar': 'Kalmar', 'kronoberg': 'Kronoberg', 'kronobergs': 'Kronoberg', 'norrbotten': 'Norrbotten', 'norrbottens': 'Norrbotten',\n",
    "        'ostgota': 'Östgöta', 'östgöta': 'Östgöta', 'skaraborg': 'Skaraborg', 'skane': 'Skåne', 'skåne': 'Skåne',\n",
    "        'sodermanland': 'Södermanland', 'södermanland': 'Södermanland', 'södermanlands': 'Södermanland', 'sörmland': 'Södermanland',\n",
    "        'stockholm': 'Stockholm', 'stockholms': 'Stockholm', 'uppsala': 'Uppsala', 'varmland': 'Värmland', 'värmland': 'Värmland',\n",
    "        'värmlands': 'Värmland', 'vasterbotten': 'Västerbotten', 'västerbotten': 'Västerbotten', 'västerbottens': 'Västerbotten',\n",
    "        'vasternorrland': 'Västernorrland', 'västernorrland': 'Västernorrland', 'västernorrlands': 'Västernorrland',\n",
    "    }\n",
    "    if region_lower in special_cases:\n",
    "        cleaned_region = special_cases[region_lower]\n",
    "        return cleaned_region if cleaned_region in VALID_REGIONS_SET else 'Övrigt'\n",
    "    for valid_region in VALID_REGIONS:\n",
    "        if region_lower == valid_region.lower(): return valid_region\n",
    "    return 'Övrigt'\n",
    "\n",
    "def safe_to_numeric(series, fill_value=0.0):\n",
    "    \"\"\"Convert series to numeric, handling comma decimals and filling errors.\"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(series): return series.fillna(fill_value)\n",
    "    numeric_series = pd.to_numeric(series.astype(str).str.replace(',', '.', regex=False), errors='coerce')\n",
    "    return numeric_series.fillna(fill_value)\n",
    "\n",
    "def standardize_columns(df, column_mapping):\n",
    "    \"\"\"Standardize column names based on a mapping dictionary.\"\"\"\n",
    "    reverse_mapping = {name.lower().strip(): std_name for std_name, names in column_mapping.items() for name in names}\n",
    "    rename_dict = {}\n",
    "    for col in df.columns:\n",
    "        col_lower_stripped = str(col).lower().strip()\n",
    "        if col_lower_stripped in reverse_mapping:\n",
    "            rename_dict[col] = reverse_mapping[col_lower_stripped]\n",
    "    return df.rename(columns=rename_dict)\n",
    "\n",
    "def get_swedish_holidays(start_year, end_year):\n",
    "    \"\"\"Get Swedish holidays for specified years, excluding weekends\"\"\"\n",
    "    try:\n",
    "        valid_years = [y for y in range(start_year, end_year + 1)]\n",
    "        if not valid_years:\n",
    "            return pd.DatetimeIndex([])\n",
    "        \n",
    "        print(f\"Fetching Swedish holidays for years: {valid_years}\")\n",
    "        \n",
    "        # Create holidays instance with specific filters\n",
    "        se_holidays = holidays.SE(years=valid_years)\n",
    "        \n",
    "        # Filter to only include official holidays (not weekends)\n",
    "        holiday_dates = []\n",
    "        for date, name in se_holidays.items():\n",
    "            # Skip if it's just a weekend without a specific holiday name\n",
    "            if name and not name.startswith('Söndag') and not name.startswith('Lördag'):\n",
    "                holiday_dates.append(date)\n",
    "        \n",
    "        holiday_dates = pd.DatetimeIndex(sorted(holiday_dates))\n",
    "        print(f\"Found {len(holiday_dates)} official holiday dates.\")\n",
    "        return holiday_dates\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not fetch Swedish holidays: {e}. Returning empty list.\")\n",
    "        return pd.DatetimeIndex([])\n",
    "\n",
    "\n",
    "# --- Primary Data Processing Function ---\n",
    "def load_and_process_file(file_key, file_info, base_dates_df):\n",
    "    \"\"\"Loads, cleans, processes, and aggregates/pivots a single data file.\"\"\"\n",
    "    file_path = DATA_DIR / file_info['name']\n",
    "    source_name = file_key.capitalize()\n",
    "    print(f\"\\n--- Processing {source_name} ({file_info['name']}) ---\")\n",
    "    try:\n",
    "        # Load Data\n",
    "        if file_info['type'] == 'excel': df = pd.read_excel(file_path)\n",
    "        elif file_info['type'] == 'csv': df = pd.read_csv(file_path, encoding=file_info['encoding'], sep=file_info['sep'])\n",
    "        else: print(f\"ERROR ({source_name}): Unknown file type.\"); return None\n",
    "        print(f\"Info ({source_name}): Loaded {len(df)} rows.\")\n",
    "        if df.empty: return None\n",
    "\n",
    "        # Standardize Columns\n",
    "        df = standardize_columns(df, STD_COL_MAPPINGS)\n",
    "        if 'Date' not in df.columns:\n",
    "             print(f\"ERROR ({source_name}): 'Date' column not found after standardization.\"); return None\n",
    "\n",
    "        # Clean Date\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.dropna(subset=['Date'])\n",
    "        df['Date'] = df['Date'].dt.normalize()\n",
    "        df = df[(df['Date'] >= START_DATE) & (df['Date'] <= END_DATE)]\n",
    "        if df.empty: print(f\"Warning ({source_name}): No data within date range.\"); return None\n",
    "        print(f\"Info ({source_name}): {len(df)} rows remain after date cleaning/filtering.\")\n",
    "\n",
    "        # Clean Region (if applicable)\n",
    "        is_regional = 'Region' in df.columns and file_info.get('is_regional', False)\n",
    "        if is_regional:\n",
    "            print(f\"Info ({source_name}): Cleaning regions...\")\n",
    "            df['Region'] = df['Region'].apply(clean_region)\n",
    "            df = df[df['Region'].isin(VALID_REGIONS_SET)]\n",
    "            print(f\"Info ({source_name}): {len(df)} rows remain after region cleaning/filtering.\")\n",
    "            if df.empty: print(f\"Warning ({source_name}): No data remains after filtering for valid regions.\"); return None\n",
    "\n",
    "        # Clean Metrics\n",
    "        for metric_col in file_info.get('metrics', []):\n",
    "            if metric_col in df.columns:\n",
    "                # Use fill_value=0 for metrics that will be summed/pivoted\n",
    "                df[metric_col] = safe_to_numeric(df[metric_col], fill_value=0.0)\n",
    "            else: print(f\"Warning ({source_name}): Expected metric column '{metric_col}' not found.\")\n",
    "\n",
    "        # Aggregation / Pivoting Logic\n",
    "        agg_cols = file_info.get('metrics', [])\n",
    "        group_by_cols = ['Date'] + file_info.get('grouping_cols', [])\n",
    "\n",
    "        # Handle National Indicators (CPI, Fuel) separately - no sum aggregation needed\n",
    "        if file_key in ['cpi', 'fuel']:\n",
    "            metric = 'CPI' if file_key == 'cpi' else 'Gas_Price'\n",
    "            if metric in df.columns:\n",
    "                processed_df = df[['Date', metric]].dropna().drop_duplicates('Date', keep='last').sort_values('Date')\n",
    "                print(f\"Info ({source_name}): Processed as national indicator. {len(processed_df)} unique date entries.\")\n",
    "                return processed_df\n",
    "            else:\n",
    "                print(f\"Warning ({source_name}): Expected indicator column '{metric}' not found.\")\n",
    "                return None\n",
    "\n",
    "        # --- Aggregate regional/channel data to national daily ---\n",
    "        agg_cols_present = [col for col in agg_cols if col in df.columns]\n",
    "        if not agg_cols_present:\n",
    "             print(f\"ERROR ({source_name}): None of the expected metric columns {agg_cols} found for aggregation.\")\n",
    "             return None\n",
    "\n",
    "        print(f\"Info ({source_name}): Aggregating {agg_cols_present} by {group_by_cols}...\")\n",
    "        grouped_df = df.groupby(group_by_cols, as_index=False)[agg_cols_present].sum()\n",
    "\n",
    "        # --- Pivot Marketing Data ---\n",
    "        if file_key == 'marketing':\n",
    "            print(f\"Info ({source_name}): Pivoting marketing data...\")\n",
    "            if 'Channel' not in grouped_df.columns:\n",
    "                 print(f\"ERROR ({source_name}): 'Channel' column needed for pivoting not found.\"); return None\n",
    "            if not all(m in grouped_df.columns for m in ['Marketing_Impressions', 'Cost']):\n",
    "                 print(f\"ERROR ({source_name}): 'Marketing_Impressions' or 'Cost' column missing before pivot.\"); return None\n",
    "\n",
    "            grouped_df['Channel'] = grouped_df['Channel'].fillna('Unknown').astype(str)\n",
    "\n",
    "            try:\n",
    "                # Pivot using the INTERNALLY standardized metric names\n",
    "                pivot_df = grouped_df.pivot_table(\n",
    "                    index='Date',\n",
    "                    columns='Channel',\n",
    "                    values=['Marketing_Impressions', 'Cost'], # Pivot these two\n",
    "                    fill_value=0\n",
    "                )\n",
    "                # Flatten MultiIndex columns and RENAME metrics as requested\n",
    "                # e.g., ('Marketing_Impressions', 'SEM') -> 'SEM_Impressions'\n",
    "                # e.g., ('Cost', 'Display') -> 'Display_Cost'\n",
    "                new_cols = []\n",
    "                for metric, channel in pivot_df.columns:\n",
    "                    if metric == 'Marketing_Impressions':\n",
    "                        new_cols.append(f'{channel}_Impressions')\n",
    "                    elif metric == 'Cost':\n",
    "                        new_cols.append(f'{channel}_Cost')\n",
    "                    else: # Fallback, should not happen with values specified\n",
    "                        new_cols.append(f'{channel}_{metric}')\n",
    "                pivot_df.columns = new_cols\n",
    "\n",
    "                processed_df = pivot_df.reset_index()\n",
    "                print(f\"Info ({source_name}): Pivoting successful. Columns: {processed_df.columns.tolist()}\")\n",
    "            except Exception as pivot_error:\n",
    "                 print(f\"ERROR ({source_name}): Failed to pivot marketing data: {pivot_error}\"); traceback.print_exc(); return None\n",
    "        else:\n",
    "            # For non-marketing files, the grouped_df is the final daily national aggregate\n",
    "            processed_df = grouped_df\n",
    "\n",
    "        print(f\"Info ({source_name}): Processing complete. Result has {len(processed_df)} rows.\")\n",
    "        return processed_df\n",
    "\n",
    "    except FileNotFoundError: print(f\"ERROR ({source_name}): File not found at {file_path}\"); return None\n",
    "    except Exception as e: print(f\"ERROR ({source_name}): Unexpected error: {e}\"); traceback.print_exc(); return None\n",
    "\n",
    "\n",
    "# %% ----- Load and Process All Files -----\n",
    "FILES_INFO = {\n",
    "    'transfers': {'name': FILE_NAMES['transfers'], 'type': 'excel', 'is_regional': True, 'metrics': ['Ownership_Transfers']},\n",
    "    'cpi': {'name': FILE_NAMES['cpi'], 'type': 'excel', 'is_regional': False, 'metrics': ['CPI']},\n",
    "    'sales': {'name': FILE_NAMES['sales'], 'type': 'csv', 'is_regional': True, 'encoding': 'utf-8', 'sep': ';', 'metrics': ['Sales']},\n",
    "    'email': {'name': FILE_NAMES['email'], 'type': 'csv', 'is_regional': True, 'encoding': 'utf-8', 'sep': ';', 'metrics': ['Email_Impressions']},\n",
    "    'marketing': {'name': FILE_NAMES['marketing'], 'type': 'csv', 'is_regional': True, 'encoding': 'ISO-8859-1', 'sep': ';',\n",
    "                  'metrics': ['Marketing_Impressions', 'Cost'], # Aggregated before pivot\n",
    "                  'grouping_cols': ['Channel']},\n",
    "    'fuel': {'name': FILE_NAMES['fuel'], 'type': 'excel', 'is_regional': False, 'metrics': ['Gas_Price']}\n",
    "}\n",
    "\n",
    "print(\"\\n--- Initializing Base Daily DataFrame ---\")\n",
    "daily_dates = pd.date_range(start=START_DATE, end=END_DATE, freq='D')\n",
    "national_daily_df = pd.DataFrame({'Date': daily_dates}).sort_values('Date')\n",
    "\n",
    "processed_data = {}\n",
    "for key, info in FILES_INFO.items():\n",
    "    processed = load_and_process_file(key, info, national_daily_df)\n",
    "    if processed is not None: processed_data[key] = processed\n",
    "\n",
    "print(\"\\n--- Merging Processed Data ---\")\n",
    "for key, df_processed in processed_data.items():\n",
    "    merge_cols = list(df_processed.columns) # Get columns to merge\n",
    "    if key in ['cpi', 'fuel']:\n",
    "        print(f\"Merging {key} using merge_asof...\")\n",
    "        national_daily_df = pd.merge_asof(national_daily_df, df_processed, on='Date', direction='backward')\n",
    "    elif key == 'marketing':\n",
    "         print(f\"Merging {key} using left merge...\")\n",
    "         # Only merge pivoted columns (Date + Channel_Metric columns)\n",
    "         national_daily_df = pd.merge(national_daily_df, df_processed, on='Date', how='left')\n",
    "    else: # Standard merge for other daily aggregates\n",
    "         print(f\"Merging {key} using left merge...\")\n",
    "         # Ensure we don't merge the 'Region' column if it accidentally remained\n",
    "         cols_to_merge = [col for col in merge_cols if col != 'Region']\n",
    "         national_daily_df = pd.merge(national_daily_df, df_processed[cols_to_merge], on='Date', how='left')\n",
    "\n",
    "print(f\"\\nColumns after merging: {national_daily_df.columns.tolist()}\")\n",
    "\n",
    "# %% ----- Feature Engineering (Daily Flags) -----\n",
    "print(\"\\n--- Generating Daily Flags ---\")\n",
    "if 'Date' not in national_daily_df.columns: # Should not happen if initialized correctly\n",
    "    national_daily_df = national_daily_df.reset_index() # Try resetting if index is Date\n",
    "if 'Date' not in national_daily_df.columns:\n",
    "     raise ValueError(\"Critical Error: 'Date' column lost before setting index.\")\n",
    "\n",
    "national_daily_df = national_daily_df.set_index('Date')\n",
    "\n",
    "# Replace the existing holiday generation code with this version\n",
    "print(\"Generating Holiday flags...\")\n",
    "min_year, max_year = national_daily_df.index.year.min(), national_daily_df.index.year.max()\n",
    "sweden_holidays = get_swedish_holidays(min_year, max_year)\n",
    "\n",
    "# Initialize holiday column to 0 first\n",
    "national_daily_df['Is_Holiday'] = 0\n",
    "\n",
    "# Only mark actual holiday dates as 1\n",
    "if not sweden_holidays.empty:\n",
    "    holiday_dates = pd.DatetimeIndex([h for h in sweden_holidays if START_DATE <= h <= END_DATE])\n",
    "    if not holiday_dates.empty:\n",
    "        national_daily_df.loc[holiday_dates, 'Is_Holiday'] = 1\n",
    "        print(f\"Holiday flags generated. Found {len(holiday_dates)} holidays in date range.\")\n",
    "        # Debug print\n",
    "        print(f\"Sample of dates marked as holidays: {holiday_dates[:5]}\")\n",
    "else:\n",
    "    print(\"Warning: No holidays found. All dates marked as non-holidays (0).\")\n",
    "\n",
    "\n",
    "print(\"Generating Ramadan flags...\")\n",
    "national_daily_df['Is_Ramadan'] = 0\n",
    "ramadan_periods_dt = [(pd.to_datetime(s), pd.to_datetime(e)) for s, e in RAMADAN_PERIODS]\n",
    "if isinstance(national_daily_df.index, pd.DatetimeIndex):\n",
    "    for start, end in ramadan_periods_dt:\n",
    "        idx_slice = national_daily_df.index[(national_daily_df.index >= start) & (national_daily_df.index <= end)]\n",
    "        if not idx_slice.empty: national_daily_df.loc[idx_slice, 'Is_Ramadan'] = 1\n",
    "    print(f\"Ramadan flags generated. Count: {national_daily_df['Is_Ramadan'].sum()}\")\n",
    "else: print(\"ERROR: Index is not DatetimeIndex, cannot generate Ramadan flags.\")\n",
    "\n",
    "\n",
    "# %% ----- Final Filling and Weekly Aggregation -----\n",
    "print(\"\\n--- Filling Final NaNs ---\")\n",
    "\n",
    "# Identify columns: sums, indicators, flags, and PIVOTED marketing cols\n",
    "sum_cols = ['Ownership_Transfers', 'Sales', 'Email_Impressions']\n",
    "# *** Get pivoted columns dynamically ***\n",
    "pivoted_marketing_cols = [col for col in national_daily_df.columns if '_Impressions' in col or '_Cost' in col]\n",
    "indicator_cols = ['CPI', 'Gas_Price']\n",
    "flag_cols = ['Is_Holiday', 'Is_Ramadan']\n",
    "\n",
    "# Fill summed columns and PIVOTED marketing columns with 0\n",
    "cols_to_fill_zero = sum_cols + pivoted_marketing_cols\n",
    "for col in cols_to_fill_zero:\n",
    "    if col in national_daily_df.columns:\n",
    "        if national_daily_df[col].isnull().any():\n",
    "             # print(f\"Filling NaNs in summed/pivoted column '{col}' with 0.\") # Less verbose\n",
    "             national_daily_df[col] = national_daily_df[col].fillna(0)\n",
    "    else: print(f\"Warning: Expected summed/pivoted column '{col}' not present for fillna(0).\")\n",
    "\n",
    "# Fill indicator columns with ffill/bfill\n",
    "for col in indicator_cols:\n",
    "    if col in national_daily_df.columns:\n",
    "         if national_daily_df[col].isnull().any():\n",
    "             # print(f\"Filling NaNs in indicator column '{col}' with ffill/bfill.\") # Less verbose\n",
    "             national_daily_df[col] = pd.to_numeric(national_daily_df[col], errors='coerce')\n",
    "             national_daily_df[col] = national_daily_df[col].ffill().bfill()\n",
    "    else: print(f\"Warning: Expected indicator column '{col}' not present for ffill/bfill.\")\n",
    "\n",
    "# Ensure flag columns are integer and filled\n",
    "for col in flag_cols:\n",
    "    if col in national_daily_df.columns:\n",
    "         national_daily_df[col] = national_daily_df[col].fillna(0).astype(int)\n",
    "\n",
    "print(\"\\n--- Aggregating to Weekly Level ---\")\n",
    "agg_funcs = {}\n",
    "for col in national_daily_df.columns:\n",
    "    if col in indicator_cols:\n",
    "        agg_funcs[col] = 'last'\n",
    "    elif col == 'Is_Holiday':  # Special case for holidays\n",
    "        agg_funcs[col] = lambda x: 1 if any(x) else 0\n",
    "    elif col == 'Is_Ramadan':  # Keep existing max aggregation for Ramadan\n",
    "        agg_funcs[col] = 'max'\n",
    "    elif pd.api.types.is_numeric_dtype(national_daily_df[col]):\n",
    "        agg_funcs[col] = 'sum'\n",
    "\n",
    "\n",
    "if not agg_funcs: print(\"CRITICAL ERROR: No columns available for weekly aggregation.\")\n",
    "else:\n",
    "    print(f\"Weekly aggregation functions (Count: {len(agg_funcs)}): {list(agg_funcs.keys())}\") # Print keys for brevity\n",
    "    try:\n",
    "        if not isinstance(national_daily_df.index, pd.DatetimeIndex): raise TypeError(\"Index not DatetimeIndex.\")\n",
    "        weekly_agg_df = national_daily_df.resample('W-MON').agg(agg_funcs)\n",
    "        weekly_agg_df.index.name = 'Date'\n",
    "        print(f\"Weekly aggregation complete. Result shape: {weekly_agg_df.shape}\")\n",
    "        print(weekly_agg_df.head())\n",
    "\n",
    "        # %% ----- Save Results -----\n",
    "        print(\"\\n--- Saving Results ---\")\n",
    "        #output_filename_csv = DATA_DIR / 'aggregated_weekly_dat.csv' # Increment version\n",
    "        output_filename_excel = DATA_DIR / 'aggregated_weekly_data.xlsx' # Increment version\n",
    "\n",
    "        #weekly_agg_df.to_csv(output_filename_csv)\n",
    "        #print(f\"Successfully saved aggregated weekly data to {output_filename_csv.resolve()}\")\n",
    "        weekly_agg_df.to_excel(output_filename_excel)\n",
    "        print(f\"Successfully saved aggregated weekly data to {output_filename_excel.resolve()}\")\n",
    "\n",
    "    except TypeError as e: print(f\"ERROR during resampling: {e}\"); traceback.print_exc()\n",
    "    except Exception as e: print(f\"An unexpected error occurred: {e}\"); traceback.print_exc()\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee545e-d03d-4a54-a2cc-b49f3e306c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
